{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import keras.backend as K\n",
    "import talib\n",
    "import traceback\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pickle.load(open(\"PriceData.pick\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot the data.\n",
    "'''\n",
    "df['close'].plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(df,lkbk,START_IDX,max_mem,reward_function):\n",
    "    \"\"\"\n",
    "    This initialises the RL run by instantiating a new Game, \n",
    "    creating a new predictive neural network and instantiating\n",
    "    experience replay.\n",
    "    Args:\n",
    "        df: This is the data frame with the market data\n",
    "        lkbk: This is the lookback period, eg. a value of 10 means 10mins, 10hrs and 10days!\n",
    "        START_IDX: This is the starting index for the main loop, allow enough for lkbk\n",
    "        \n",
    "    Returns:\n",
    "        env: an instance of Game, our environment\n",
    "        model: the neural network\n",
    "        exp_replay: an instance of ExperienceReplay\n",
    "    \"\"\"\n",
    "\n",
    "    bars1h = df['close'].resample('1H',label='right',closed='right').ohlc().dropna()\n",
    "    bars1d = df['close'].resample('1D',label='right',closed='right').ohlc().dropna()\n",
    "    env = Game(df, bars1d,bars1h,reward_function, lkbk=lkbk, init_idx=START_IDX)\n",
    "    hidden_size = len(env.state)*HIDDEN_MULT\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(env.state), input_shape=(len(env.state),), activation=ACTIVATION_FUN))\n",
    "    model.add(Dense(hidden_size, activation=ACTIVATION_FUN))\n",
    "    model.add(Dense(NUM_ACTIONS, activation='softmax'))\n",
    "    model.compile(sgd(lr=LEARNING_RATE), loss=LOSS_FUNCTION)\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(len(env.state), input_shape=(len(env.state),), activation=ACTIVATION_FUN))\n",
    "    model2.add(Dense(hidden_size, activation=ACTIVATION_FUN))\n",
    "    model2.add(Dense(NUM_ACTIONS, activation='softmax'))\n",
    "    model2.compile(sgd(lr=LEARNING_RATE), loss=LOSS_FUNCTION)\n",
    "    \n",
    "    exp_replay = ExperienceReplay(max_memory=max_mem)\n",
    "    \n",
    "    return env,model,model2,exp_replay  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_synth_ohlc(x,y,mult=1): \n",
    "    \"\"\"\n",
    "    This creates a dataframe with sythetic open/high/low/close prices\n",
    "    based on the dates of the original price data for test and\n",
    "    comparison.\n",
    "    \"\"\"\n",
    "    op = pd.Series(y+mult*np.random.randn(len(df)),index=df.index)\n",
    "    cl = pd.Series(y+mult*np.random.randn(len(df)),index=df.index)\n",
    "    hi = pd.Series(y+1+mult*np.abs(np.random.randn(len(df))),index=df.index)\n",
    "    lo = pd.Series(y-1-mult*np.abs(np.random.randn(len(df))),index=df.index)\n",
    "    dfs = pd.DataFrame([op,hi,lo,cl]).T\n",
    "    dfs.index.rename('Time',inplace=True)\n",
    "    dfs.columns = ['open','high','low','close']\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def create_synth_time_series(df):\n",
    "    \"\"\"\n",
    "    This creates an artificial prices series for trends and/or sine waves\n",
    "    and optionally, noise can be added to this.\n",
    "    \"\"\"\n",
    "    \n",
    "    ldf = len(df)\n",
    "    mid= int(ldf/2)\n",
    "    x = np.arange(len(df))\n",
    "    y = np.zeros(ldf)\n",
    "    y[mid:] = 0.01*x[:mid]+100\n",
    "    y[:mid] = 10*np.sin(0.01*x[:mid])+100#+y[mid-1]\n",
    "    dft = create_synth_ohlc(x,y,mult=0.00)\n",
    "    return dft\n",
    "    \n",
    "\n",
    "dft = create_synth_time_series(df)\n",
    "dft['close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\" REWARD FUNCTIONS\"\"\"\n",
    "    \n",
    "    def get_pnl(entry,curr,pos):\n",
    "        return (entry - curr)/entry*pos\n",
    "    \n",
    "    def reward_func1(entry,curr,pos):\n",
    "        \"\"\"positive log categorical\"\"\"\n",
    "        pnl = get_pnl(entry,curr,pos)\n",
    "        if pnl>=0: \n",
    "            return np.ceil(np.log(pnl*100+1))\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def reward_func2(entry,curr,pos):\n",
    "        '''pure pnl'''\n",
    "        return get_pnl(entry,curr,pos)\n",
    "    \n",
    "    def reward_func3(entry,curr,pos):\n",
    "        '''positive pnl, zero otherwise'''\n",
    "        pnl = get_pnl(entry,curr,pos)\n",
    "        if pnl>=0: \n",
    "            return pnl\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def reward_func4(entry,curr,pos):\n",
    "        '''Sign of pnl'''\n",
    "        return np.sign(get_pnl(entry,curr,pos))\n",
    "    \n",
    "    def reward_func5(entry,curr,pos):\n",
    "        '''1 for win, 0 for loss'''\n",
    "        pnl = get_pnl(entry,curr,pos)\n",
    "        if pnl>=0: \n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def reward_func6(entry,curr,pos):\n",
    "        '''pure pnl'''\n",
    "        return np.exp(get_pnl(entry,curr,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "\n",
    "    def __init__(self, df,bars1d,bars1h,reward_function, lkbk=20, init_idx=None):\n",
    "        self.df = df\n",
    "        self.lkbk = lkbk\n",
    "        self.trade_len = 0\n",
    "        self.stop_pnl = None\n",
    "        self.bars1d =  bars1d\n",
    "        self.bars1h =  bars1h\n",
    "        self.is_over = False\n",
    "        self.reward = 0\n",
    "        self.pnl_sum = 0\n",
    "        self.init_idx = init_idx\n",
    "        self.reward_function = reward_function\n",
    "        self.reset()\n",
    "        \n",
    "    def _update_state(self, action):\n",
    "        \n",
    "        '''Here we update our state.\n",
    "        The state consists of the current parameters of the system,\n",
    "        similar to the current frame of a gaming screen. It includes\n",
    "        current time, price, position and reward.\n",
    "        We the add secondary features such as technical indicators in _assemble state.\n",
    "        Args:\n",
    "            action: The action suggested by the neural network based on past experience\n",
    "        '''\n",
    "        self.curr_idx += 1\n",
    "        self.curr_time = self.df.index[self.curr_idx]\n",
    "        self.curr_price = self.df['close'][self.curr_idx]\n",
    "        self.curr_low = self.df['low'][self.curr_idx]\n",
    "        self.pnl = (-self.entry + self.curr_price)*self.position/self.entry\n",
    "        self._assemble_state()\n",
    "        tm_lst = list(map(float,str(self.curr_time.time()).split(':')[:2]))\n",
    "        self._time_of_day = (tm_lst[0]*60 + tm_lst[1])/(24*60) \n",
    "        self._day_of_week  = self.curr_time.weekday()/6\n",
    "        \n",
    "        '''This is where we define our policy and update our position'''\n",
    "        if action == 0:  \n",
    "            pass\n",
    "        \n",
    "        elif action == 2:\n",
    "            \"\"\"---Enter a long or exit a short position---\"\"\"\n",
    "            if self.position == -1:\n",
    "                self.is_over = True\n",
    "                self._get_reward()\n",
    "                self.trade_len = self.curr_idx - self.start_idx\n",
    "   \n",
    "            elif self.position == 0:\n",
    "                self.position = 1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "        elif action == 1:\n",
    "            \"\"\"---Enter a short or exit a long position---\"\"\"\n",
    "            if self.position == 1:\n",
    "                self.is_over = True\n",
    "                self._get_reward()\n",
    "                self.trade_len = self.curr_idx - self.start_idx\n",
    "\n",
    "            elif self.position == 0:\n",
    "                self.position = -1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    \n",
    "    def _assemble_state(self):\n",
    "        '''Here we can add secondary features such as indicators and times to our current state.\n",
    "        First, we create candlesticks for different bar sizes of 5mins, 1hr and 1d.\n",
    "        We then add some state variables such as time of day, day of week and position.\n",
    "        Next several indicators are added and subsequently z-scored.\n",
    "        '''\n",
    "        \n",
    "        \"\"\"---Adding State Variables---\"\"\"\n",
    "        self.state = np.array([])\n",
    "        \n",
    "        \"\"\"---Adding Candlesticks---\"\"\"\n",
    "        self._get_last_N_timebars()\n",
    "        bars = [self.last5m,self.last1h,self.last1d]\n",
    "        state_bars = []\n",
    "        candles = {j:{k:np.array([]) for k in ['open','high','low','close']} for j in range(len(bars))}\n",
    "        for j,bar in enumerate(bars):\n",
    "            for col in ['open','high','low','close']:\n",
    "                candles[j][col] = np.asarray(bar[col])\n",
    "                state_bars += (list(np.asarray(bar[col]))[-10:])\n",
    "                \n",
    "        self.state = np.append(self.state,state_bars)\n",
    "\n",
    "        \"\"\"---Adding Techincal Indicators---\"\"\"\n",
    "        for c in candles:\n",
    "            try:\n",
    "                sma1 = talib.SMA(candles[c]['close'],self.lkbk-1)[-1]\n",
    "                sma2 = talib.SMA(candles[c]['close'],self.lkbk-8)[-1]\n",
    "                self.state = np.append(self.state,(sma1-sma2)/sma2)\n",
    "                self.state = np.append(self.state,talib.RSI(candles[c]['close'],self.lkbk-1)[-1])\n",
    "                self.state = np.append(self.state,talib.MOM(candles[c]['close'],self.lkbk-1)[-1])\n",
    "                self.state = np.append(self.state,talib.BOP(candles[c]['open'],\n",
    "                                               candles[c]['high'],\n",
    "                                               candles[c]['low'],\n",
    "                                               candles[c]['close'])[-1])\n",
    "\n",
    "                self.state = np.append(self.state,talib.AROONOSC(candles[c]['high'],\n",
    "                                               candles[c]['low'],\n",
    "                                               self.lkbk-3)[-1])\n",
    "            except: print(traceback.format_exc())\n",
    "                \n",
    "        \"\"\"---Normalizing Candlesticks---\"\"\"\n",
    "        self.state = (np.array(self.state)-np.mean(self.state,axis=0))/np.std(self.state,axis=0)\n",
    "        \n",
    "        self.state = np.append(self.state,self.position)\n",
    "        self.state = np.append(self.state,self._time_of_day)\n",
    "        self.state = np.append(self.state,self._day_of_week)\n",
    "        #print(list(map(len,[self.last5m,self.last1h,self.last1d])))\n",
    "        \n",
    "    def _get_last_N_timebars(self):\n",
    "        '''This function gets the timebars for the 5m, 1hr and 1d resolution based\n",
    "        on the lookback we've specified.\n",
    "        '''\n",
    "        wdw5m = 9\n",
    "        wdw1h = np.ceil(self.lkbk*15/24.)\n",
    "        wdw1d = np.ceil(self.lkbk*17)\n",
    "        \n",
    "        \"\"\"---creating the candlesticks based on windows---\"\"\"\n",
    "        self.last5m = self.df[self.curr_time-timedelta(wdw5m):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1h = self.bars1h[self.curr_time-timedelta(wdw1h):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1d = self.bars1d[self.curr_time-timedelta(wdw1d):self.curr_time].iloc[-self.lkbk:]\n",
    "        \n",
    "    def _get_reward(self):\n",
    "        \"\"\"Here we calculate the reward when the game is finished.\n",
    "        Reward function design is very difficult and can significantly\n",
    "        impact the performance of our algo.\n",
    "        In this case we use a simple pnl reward but it is conceivable to use\n",
    "        other metrics such as Sharpe ratio, average return, etc.\n",
    "        \"\"\"\n",
    "        if self.is_over:\n",
    "            self.reward = self.reward_function(self.entry,self.curr_price,self.position)\n",
    "\n",
    "\n",
    "            \n",
    "    def observe(self):\n",
    "        \"\"\"This function returns the state of the system.\n",
    "        Returns:\n",
    "            self.state: the state including indicators, position and times.\n",
    "        \"\"\"\n",
    "        return np.array([self.state])\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function updates the state based on an action\n",
    "        that was calculated by the NN.\n",
    "        This is the point where the game interacts with the trading\n",
    "        algo.\n",
    "        \"\"\"\n",
    "        self._update_state(action)\n",
    "        reward = self.reward\n",
    "        game_over = self.is_over\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resetting the system for each new trading game.\n",
    "        Here, we also resample the bars for 1h and 1d.\n",
    "        Ideally, we should do this on every update but this will take very long.\n",
    "        \"\"\"\n",
    "        self.pnl = 0\n",
    "        self.entry = 0\n",
    "        self._time_of_day = 0\n",
    "        self._day_of_week = 0\n",
    "        self.curr_idx = self.init_idx         \n",
    "        self.t_in_secs = (self.df.index[-1]-self.df.index[0]).total_seconds()\n",
    "        self.start_idx = self.curr_idx\n",
    "        self.curr_time = self.df.index[self.curr_idx]\n",
    "        self._get_last_N_timebars()\n",
    "        self.state = []\n",
    "        self.position = 0\n",
    "        self._update_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    '''This class calculates the Q-Table.\n",
    "    It gathers memory from previous experience and \n",
    "    creates a Q-Table with states and rewards for each\n",
    "    action using the NN. At the end of the game the reward\n",
    "    is calculated from the reward function. \n",
    "    The weights in the NN are constantly updated with each new\n",
    "    batch of experience. \n",
    "    This is the heart of the RL algorithm.\n",
    "    Args:\n",
    "        state_tp1: state at time t+1\n",
    "        state_t: state at time t\n",
    "        action_t: int {0..2} hold, sell, buy taken at state_t \n",
    "        Q_sa: float, reward for state_tp1\n",
    "        reward_t: reward for state_t\n",
    "        self.memory: list of state_t, action_t and reward_t at time t as well as state_tp1\n",
    "        targets: array(float) Nx2, weight of each action \n",
    "        inputs: an array with scrambled states at different times\n",
    "        targets: Nx3 array of weights for each action for scrambled input states\n",
    "    '''\n",
    "    def __init__(self, max_memory=1000, discount=DISCOUNT_RATE):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''Add states to time t and t+1 as well as  to memory'''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def process(self, modelQ, modelR, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = modelQ.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        \n",
    "        \"\"\"---Initialise input and target arrays---\"\"\"\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        \"\"\"Step randomly through different places in the memory\n",
    "        and scramble them into a new input array (inputs) with the\n",
    "        length of the pre-defined batch size\"\"\"\n",
    "        \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            \n",
    "        #for i, idx in enumerate(np.arange(-inputs.shape[0],0)):    \n",
    "            \"\"\"Obtain the parameters for Bellman from memory,\n",
    "            S.A.R.S: state, action, reward, new state.\"\"\"\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = state_t    \n",
    "            \n",
    "            \"\"\"---Calculate the targets for the state at time t---\"\"\"\n",
    "            targets[i] = modelR.predict(state_t)[0]\n",
    "            \n",
    "            \"\"\"---Calculate the reward at time t+1 for action at time t---\"\"\"\n",
    "            Q_sa = np.max(modelQ.predict(state_tp1)[0])\n",
    "           \n",
    "            if game_over:\n",
    "                \"\"\"---When game is over we have a definite reward---\"\"\"\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                \"\"\"\n",
    "                ---Update the part of the target for which action_t occured to new value---\n",
    "                Q_new(s,a) = reward_t + gamma * max_a' Q(s', a')\n",
    "                \"\"\"\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        \n",
    "      \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(df,preload=False,update_qr=True,pnl_file=PNL_FILE,reward_function=reward_func2):\n",
    "    pnls = []\n",
    "    pnl_dates = []\n",
    "    e = 0\n",
    "\n",
    "    \"\"\"---Initialise a NN and a set up initial game parameters---\"\"\"\n",
    "    env,q_network,r_network,exp_replay = init_net(df,LKBK,START_IDX,MAX_MEM,reward_function)\n",
    "    \n",
    "    \"\"\"---Preloading the model weights---\"\"\"\n",
    "    if preload:\n",
    "        q_network.load_weights(WEIGHTS_FILE)\n",
    "        r_network.load_weights(WEIGHTS_FILE)\n",
    "        \n",
    "    r_network.set_weights(q_network.get_weights())\n",
    "        \n",
    "    bars1h = df['close'].resample('1H',label='right',closed='right').ohlc().dropna()\n",
    "    bars1d = df['close'].resample('1D',label='right',closed='right').ohlc().dropna()\n",
    "    \n",
    "\n",
    "    \"\"\"---Loop that steps through one trade (game) at a time---\"\"\"\n",
    "    while True:\n",
    "        \n",
    "        \"\"\"---Stop the algo when end is near to avoid exception---\"\"\"\n",
    "        if env.curr_idx >= len(df)-1000:#len(df)+60000:\n",
    "            break\n",
    "        \n",
    "        e += 1\n",
    "        \n",
    "        \"\"\"---Initialise a new game---\"\"\"\n",
    "        env = Game(df, bars1d,bars1h,reward_function,lkbk=LKBK, init_idx=env.curr_idx)\n",
    "        \n",
    "        \"\"\"---Calculate epsilon for exploration vs exploitation random action generator---\"\"\"\n",
    "        epsilon = EPSILON**(np.log10(e))+EPS_MIN\n",
    "         \n",
    "\n",
    "        \n",
    "        game_over = False\n",
    "        state_tp1 = env.observe()\n",
    "        cnt = 0\n",
    "        \n",
    "        \"\"\"---Walk through time steps starting from the end of the last game---\"\"\"\n",
    "        while not game_over:\n",
    "            cnt += 1\n",
    "            state_t = state_tp1\n",
    "            \n",
    "            #\"\"\"---Generate a random action---\"\"\"\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, 3, size=1)[0]\n",
    "            \n",
    "            #\"\"\"---Action for opening a trade---\"\"\"\n",
    "            else:\n",
    "                q = q_network.predict(state_t)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            #\"\"\"---Updating the Game---\"\"\"\n",
    "            state_tp1, reward, game_over = env.act(action)\n",
    "            \n",
    "            #\"\"\"---Adding state to memory---\"\"\"\n",
    "            #print(env.curr_time,len(state_t[0]),len(state_tp1[0]))\n",
    "            exp_replay.remember([state_t, action, reward, state_tp1], game_over)\n",
    "\n",
    "            #\"\"\"---Creating a new Q-Table---\"\"\"\n",
    "            inputs, targets = exp_replay.process(q_network, r_network, batch_size=BATCH_SIZE)\n",
    "            env.pnl_sum = sum(pnls)\n",
    "\n",
    "            #\"\"\"---Update the NN model with a new Q-Table\"\"\"\n",
    "            q_network.train_on_batch(inputs, targets)\n",
    "            if game_over and update_qr:\n",
    "            #if env.curr_idx<3000 or not env.curr_idx%1000:\n",
    "               r_network.set_weights(q_network.get_weights())\n",
    "                \n",
    "            #\"\"\"---This is a stop but it ensures that the cycle runs until the end---\"\"\"\n",
    "#             if env.pnl<=-0.1:\n",
    "#                 if not env.stop_pnl: \n",
    "#                     print('STOP LOSS; len: %s'%(env.curr_idx-env.start_idx))\n",
    "                                                                    \n",
    "#                 env.stop_pnl = -0.1\n",
    "                \n",
    "                \n",
    "        \n",
    "        if env.stop_pnl:\n",
    "            pnls.append(env.stop_pnl)\n",
    "        else:\n",
    "            pnls.append(env.pnl)\n",
    "        pnl_dates.append(env.curr_time)\n",
    "        pickle.dump([pnl_dates,pnls],open(pnl_file,'wb'))\n",
    "        \n",
    "        print(\"Trade {:03d} | pos {} | len {} | pnl {:,.2f}% | eps {:,.4f} | {} | {}\".format(e,  \n",
    "                                                                              env.position, \n",
    "                                                                              env.trade_len,\n",
    "                                                                              sum(pnls)*100,\n",
    "                                                                              epsilon,\n",
    "                                                                              env.curr_time,\n",
    "                                                                              env.curr_idx))\n",
    "\n",
    "\n",
    "        \n",
    "        if not e%10:\n",
    "            print('----saving weights-----')\n",
    "            r_network.save_weights(WEIGHTS_FILE, overwrite=True)\n",
    "\n",
    "    print('***FINISHED***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CONFIG\"\"\"\n",
    "LEARNING_RATE = 0.001\n",
    "LOSS_FUNCTION = 'mse'\n",
    "ACTIVATION_FUN = 'relu'\n",
    "NUM_ACTIONS = 3\n",
    "HIDDEN_MULT = 2\n",
    "DISCOUNT_RATE = 0.99\n",
    "LKBK = 30\n",
    "BATCH_SIZE = 1\n",
    "MAX_MEM = 600\n",
    "EPSILON = 0.01\n",
    "EPS_MIN = 0.001\n",
    "START_IDX = 5000\n",
    "WEIGHTS_FILE = \"indicator_model.h5\"\n",
    "PNL_FILE = 'pnls.pick'\n",
    "RF = reward_func6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print('==================TRAIN RUN %s=============='%i)\n",
    "#     dfn = deepcopy(dft)\n",
    "#     run(dfn, preload=False,update_qr=True,pnl_file='pnls%s.pick'%i)\n",
    "run(df, \n",
    "       preload=False,\n",
    "       update_qr=True,\n",
    "       reward_function=RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
