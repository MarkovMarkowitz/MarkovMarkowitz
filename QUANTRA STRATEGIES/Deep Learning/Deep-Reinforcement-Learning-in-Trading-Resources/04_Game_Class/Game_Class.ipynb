{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXBnWkIb6Q_z"
   },
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQICc9Vt6Q_2"
   },
   "source": [
    "# Game Class\n",
    "\n",
    "In this notebook, you will learn to create a full `Game` class. In the previous notebooks, you have learned to update the positions, design reward system and assemble the states. Here, you will put these inside the Game class.\n",
    "\n",
    "You will perform the following steps:\n",
    "\n",
    "1. [Import libraries](#libraries)\n",
    "2. [Read price data](#price)\n",
    "3. [Design reward](#reward)\n",
    "4. [Construct Game class](#game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5s1oCJT6Q_5"
   },
   "source": [
    "<a id='libraries'></a> \n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pIn6vI9Y6Q_7"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib\n",
    "from datetime import datetime, timedelta\n",
    "import datetime\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8bFGZ-j6RAW"
   },
   "source": [
    "<a id='price'></a> \n",
    "## Read price data\n",
    "\n",
    "We will read the 5 minutes price data from the compressed pickle file. You have already done these steps in the previous notebooks. You can find the data file in the last section of this course **\"Python Data and Codes\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "W92DUIXL6RAX",
    "outputId": "8825c7bf-8fd3-406a-a2a3-9590aff2d60d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:35:00-05:00</th>\n",
       "      <td>91.711</td>\n",
       "      <td>91.809</td>\n",
       "      <td>91.703</td>\n",
       "      <td>91.760</td>\n",
       "      <td>4448908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:40:00-05:00</th>\n",
       "      <td>91.752</td>\n",
       "      <td>91.973</td>\n",
       "      <td>91.752</td>\n",
       "      <td>91.932</td>\n",
       "      <td>4380988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:45:00-05:00</th>\n",
       "      <td>91.940</td>\n",
       "      <td>92.022</td>\n",
       "      <td>91.928</td>\n",
       "      <td>92.005</td>\n",
       "      <td>2876633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:50:00-05:00</th>\n",
       "      <td>92.005</td>\n",
       "      <td>92.177</td>\n",
       "      <td>91.973</td>\n",
       "      <td>92.177</td>\n",
       "      <td>4357079.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:55:00-05:00</th>\n",
       "      <td>92.168</td>\n",
       "      <td>92.177</td>\n",
       "      <td>92.038</td>\n",
       "      <td>92.079</td>\n",
       "      <td>2955068.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             open    high     low   close     volume\n",
       "Time                                                                \n",
       "2010-01-04 09:35:00-05:00  91.711  91.809  91.703  91.760  4448908.0\n",
       "2010-01-04 09:40:00-05:00  91.752  91.973  91.752  91.932  4380988.0\n",
       "2010-01-04 09:45:00-05:00  91.940  92.022  91.928  92.005  2876633.0\n",
       "2010-01-04 09:50:00-05:00  92.005  92.177  91.973  92.177  4357079.0\n",
       "2010-01-04 09:55:00-05:00  92.168  92.177  92.038  92.079  2955068.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data is stored in the directory 'data'\n",
    "path = '../data_modules/'\n",
    "\n",
    "bars5m = pd.read_pickle(path+ 'PriceData5m.bz2')\n",
    "\n",
    "bars5m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX3P7gIr6RAl"
   },
   "source": [
    "<a id='reward'></a> \n",
    "## Design reward \n",
    "\n",
    "You have already designed the reward system based on the pnl in the \"Reward Design\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hd5rdF546RAm"
   },
   "outputs": [],
   "source": [
    "def get_pnl(entry, curr_price, position):\n",
    "    # Transaction cost and commissions\n",
    "    tc = 0.001\n",
    "    return (curr_price*(1-tc) - entry*(1+tc))/entry*(1+tc)*position\n",
    "\n",
    "\n",
    "def reward_pure_pnl(entry, curr_price, position):\n",
    "    '''pure pnl'''\n",
    "    return get_pnl(entry, curr_price, position)\n",
    "\n",
    "\n",
    "def reward_positive_pnl(entry, curr_price, position):\n",
    "    '''Positive pnl, zero otherwise'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "\n",
    "    if pnl >= 0:\n",
    "        return pnl\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def reward_pos_log_pnl(entry, curr_price, position):\n",
    "    '''Positive log pnl, zero otherwise'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "\n",
    "    if pnl >= 0:\n",
    "        return np.ceil(np.log(pnl*100+1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def reward_categorical_pnl(entry, curr_price, position):\n",
    "    '''Sign of pnl'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "    return np.sign(pnl)\n",
    "\n",
    "\n",
    "def reward_positive_categorical_pnl(entry, curr_price, position):\n",
    "    '''1 for win, 0 for loss'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "    if pnl >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def reward_exponential_pnl(entry, curr_price, position):\n",
    "    '''Exponentual percentage pnl'''\n",
    "    pnl = get_pnl(entry, curr_price, position)\n",
    "    return np.exp(pnl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS4-LgXG6RAu"
   },
   "source": [
    "<a id='game'></a> \n",
    "## Construct Game class\n",
    "\n",
    "You have done the majority of the steps from this class such as initialising the Game class, updating the position, calculating the reward, creating the input features and assembling the state. \n",
    "\n",
    "We are adding two more functions to complete the Game class.\n",
    "\n",
    "`get_state`: This function returns the state of the system, including candlesticks, indicators, day of the week, time of the day and position.\n",
    "            \n",
    "`act`: This function interacts with the trading algorithm. It takes action as a parameter suggested by the neural networks and returns a flag whether game is over or not and a reward when game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidePrompt": true,
    "id": "lWVOiLA42G7I"
   },
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "\n",
    "    def __init__(self, bars5m, bars1d, bars1h, reward_function, lkbk=20,  init_idx=None):\n",
    "\n",
    "        # Initialise 5 mins frequency data\n",
    "        self.bars5m = bars5m\n",
    "        # Initilaise lookback period for the calculation of technical indicators\n",
    "        self.lkbk = lkbk\n",
    "        # Intialise length of each trade\n",
    "        self.trade_len = 0\n",
    "        # Initialise 1 day frequency data\n",
    "        self.bars1d = bars1d\n",
    "        # Initialise 1 hour frequency data\n",
    "        self.bars1h = bars1h\n",
    "        # Initialise when game is over to update the state, position and calculate reward\n",
    "        self.is_over = False\n",
    "        # Intialise reward to store the value of reward\n",
    "        self.reward = 0\n",
    "        # Define pnl_sum to calculate the pnl when all episodes are complete.\n",
    "        self.pnl_sum = 0\n",
    "        # Supply a starting index which indicates a position in our price dataframe\n",
    "        # and denotes the point at which the game starts\n",
    "        self.init_idx = init_idx\n",
    "        # Instantiate reward function\n",
    "        self.reward_function = reward_function\n",
    "        # When game is over, reset all state values\n",
    "        self.reset()\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _update_position(self, action):\n",
    "        '''This is where we update our position'''\n",
    "\n",
    "        # If the action is zero or hold, do nothing\n",
    "        if action == 0:\n",
    "            pass\n",
    "\n",
    "        elif action == 2:\n",
    "            \"\"\"---Enter a long or exit a short position---\"\"\"\n",
    "\n",
    "            # Current position (long) same as the action (buy), do nothing\n",
    "            if self.position == 1:\n",
    "                pass\n",
    "\n",
    "            # No current position, and action is buy, update the position to indicate buy\n",
    "            elif self.position == 0:\n",
    "                self.position = 1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "\n",
    "            # Current postion (short) is different than the action (buy), end the game\n",
    "            elif self.position == -1:\n",
    "                self.is_over = True\n",
    "\n",
    "        elif action == 1:\n",
    "            \"\"\"---Enter a short or exit a long position---\"\"\"\n",
    "\n",
    "            # Current position (short) same as the action (sell), do nothing\n",
    "            if self.position == -1:\n",
    "                pass\n",
    "\n",
    "            # No current position, and action is sell, update the position to indicate sell\n",
    "            elif self.position == 0:\n",
    "                self.position = -1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "\n",
    "            # Current postion (long) is different than the action (sell), end the game\n",
    "            elif self.position == 1:\n",
    "                self.is_over = True\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"Here we calculate the reward when the game is finished.\n",
    "        In this case, we use a exponential pnl reward.\n",
    "        \"\"\"\n",
    "        if self.is_over:\n",
    "            self.reward = self.reward_function(\n",
    "                self.entry, self.curr_price, self.position)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _get_last_N_timebars(self):\n",
    "        '''This function gets the timebars for the 5 mins, 1 hour and 1 day resolution based on the lookback we've specified.'''\n",
    "\n",
    "        '''Width of the 5m, 1hr, and 1d'''\n",
    "        self.wdw5m = 9\n",
    "        self.wdw1h = np.ceil(self.lkbk*15/24.)\n",
    "        self.wdw1d = np.ceil(self.lkbk*15)\n",
    "\n",
    "        '''Creating the candlesticks based on windows'''\n",
    "        self.last5m = self.bars5m[self.curr_time -\n",
    "                                  timedelta(self.wdw5m):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1h = self.bars1h[self.curr_time -\n",
    "                                  timedelta(self.wdw1h):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1d = self.bars1d[self.curr_time -\n",
    "                                  timedelta(self.wdw1d):self.curr_time].iloc[-self.lkbk:]\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def _assemble_state(self):\n",
    "        self._get_last_N_timebars()\n",
    "\n",
    "        \"\"\"Adding State Variables\"\"\"\n",
    "        self.state = np.array([])\n",
    "\n",
    "        \"\"\"Adding candlesticks\"\"\"\n",
    "        def get_normalised_bars_array(bars):\n",
    "            bars = bars.iloc[-10:].values.flatten()\n",
    "            bars = (bars-np.mean(bars))/np.std(bars)\n",
    "            return bars\n",
    "\n",
    "        self.state = np.append(self.state, get_normalised_bars_array(\n",
    "            self.last5m[['open', 'high', 'low', 'close']]))\n",
    "        self.state = np.append(\n",
    "            self.state, get_normalised_bars_array(self.last1h))\n",
    "        self.state = np.append(\n",
    "            self.state, get_normalised_bars_array(self.last1d))\n",
    "\n",
    "        \"\"\"\" Adding technical indicators\"\"\"\n",
    "        def get_technical_indicators(bars):\n",
    "            # Create an array to store the value of indicators\n",
    "            tech_ind = np.array([])\n",
    "\n",
    "            \"\"\"Relative difference two moving averages\"\"\"\n",
    "            sma1 = talib.SMA(bars['close'], self.lkbk-1)[-1]\n",
    "            sma2 = talib.SMA(bars['close'], self.lkbk-8)[-1]\n",
    "            tech_ind = np.append(tech_ind, (sma1-sma2)/sma2)\n",
    "\n",
    "            \"\"\"Relative Strength Index\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.RSI(\n",
    "                bars['close'], self.lkbk-1)[-1])\n",
    "\n",
    "            \"\"\"Momentum\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.MOM(\n",
    "                bars['close'], self.lkbk-1)[-1])\n",
    "\n",
    "            \"\"\"Balance of Power\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.BOP(bars['open'],\n",
    "                                                     bars['high'],\n",
    "                                                     bars['low'],\n",
    "                                                     bars['close'])[-1])\n",
    "\n",
    "            \"\"\"Aroon Oscillator\"\"\"\n",
    "            tech_ind = np.append(tech_ind, talib.AROONOSC(bars['high'],\n",
    "                                                          bars['low'],\n",
    "                                                          self.lkbk-3)[-1])\n",
    "            return tech_ind\n",
    "\n",
    "        self.state = np.append(\n",
    "            self.state, get_technical_indicators(self.last5m))\n",
    "        self.state = np.append(\n",
    "            self.state, get_technical_indicators(self.last1h))\n",
    "        self.state = np.append(\n",
    "            self.state, get_technical_indicators(self.last1d))\n",
    "\n",
    "        \"\"\"Time of the day and day of the week\"\"\"\n",
    "        tm_lst = list(map(float, str(self.curr_time.time()).split(':')[:2]))\n",
    "        _time_of_day = (tm_lst[0]*60 + tm_lst[1])/(24*60)\n",
    "        _day_of_week = self.curr_time.weekday()/6\n",
    "\n",
    "        self.state = np.append(self.state, self._time_of_day)\n",
    "        self.state = np.append(self.state, self._day_of_week)\n",
    "        self.state = np.append(self.state, self.position)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"This function returns the state of the system.\n",
    "        Returns:\n",
    "            self.state: the state including candlestick bars, indicators, time signatures and position.\n",
    "        \"\"\"\n",
    "        # Assemble new state\n",
    "        self._assemble_state()\n",
    "        return np.array([self.state])\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This is the point where the game interacts with the trading\n",
    "        algo. It returns value of reward when game is over.\n",
    "        \"\"\"\n",
    "\n",
    "        self.curr_time = self.bars5m.index[self.curr_idx]\n",
    "        self.curr_price = self.bars5m['close'][self.curr_idx]\n",
    "\n",
    "        self._update_position(action)\n",
    "\n",
    "        # Unrealized or realized pnl. This is different from pnl in reward method which is only realized pnl.\n",
    "        self.pnl = (-self.entry + self.curr_price)*self.position/self.entry\n",
    "\n",
    "        self._get_reward()\n",
    "        if self.is_over:\n",
    "            self.trade_len = self.curr_idx - self.start_idx\n",
    "\n",
    "        return self.is_over, self.reward\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resetting the system for each new trading game.\n",
    "        Here, we also resample the bars for 1h and 1d.\n",
    "        Ideally, we should do this on every update but this will take very long.\n",
    "        \"\"\"\n",
    "        self.pnl = 0\n",
    "        self.entry = 0\n",
    "        self._time_of_day = 0\n",
    "        self._day_of_week = 0\n",
    "        self.curr_idx = self.init_idx\n",
    "        self.t_in_secs = (\n",
    "            self.bars5m.index[-1]-self.bars5m.index[0]).total_seconds()\n",
    "        self.start_idx = self.curr_idx\n",
    "        self.curr_time = self.bars5m.index[self.curr_idx]\n",
    "        self._get_last_N_timebars()\n",
    "        self.position = 0\n",
    "        self.act(0)\n",
    "        self.state = []\n",
    "        self._assemble_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PKa7R5vk6RA5",
    "outputId": "52466a9a-542b-4cbf-9570-34cd4aca5fc5"
   },
   "outputs": [],
   "source": [
    "ohlcv_dict = {\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "}\n",
    "\n",
    "# Resample 5 mins data to 1 hour data\n",
    "bars1h = bars5m.resample(\n",
    "    '1H', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "\n",
    "# Reample 5 mins data to daily data\n",
    "bars1d = bars5m.resample(\n",
    "    '1D', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "\n",
    "# Create Game class environment\n",
    "env = Game(bars5m, bars1d, bars1h, reward_exponential_pnl,\n",
    "           lkbk=10,  init_idx=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxus8mLX6RBB"
   },
   "source": [
    "### Analyse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gYlCtBzE6RBC",
    "outputId": "3ea9458c-de72-4c09-af39-20828e1c9ed6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.act(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N5HX3oA6RBH"
   },
   "source": [
    "We passed action = 1 or sell as an input to the act() method. It returns False, which is a flag for the game over or not. Since the game is not over, it returns no reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9nPW6LJA6RBH",
    "outputId": "894788f5-3c5c-4965-aaa9-633abbf01600"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1.0020040053400068)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.act(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZMZIz2r6RBK"
   },
   "source": [
    "Now, we passed action = 2 or buy, which is opposite of the previous action sell. Therefore, the game is over, and you get the reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWRpPUuW6RBL"
   },
   "source": [
    "The neural networks or the agent suggests the actions. Here, for the illustration purpose, we passed the action and observed the output of the Game class. \n",
    "\n",
    "You can tweak the code and change the parameters of the Game class such as lookback period or can change the reward function other than exponential pnl.\n",
    "\n",
    "In the coming section, you will learn to model the agent, which is another essential part of the reinforcement learning \n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Game Class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
