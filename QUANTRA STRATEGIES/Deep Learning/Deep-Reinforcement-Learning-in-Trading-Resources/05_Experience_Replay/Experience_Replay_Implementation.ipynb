{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6qOOFbuBrKf"
   },
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8YgAx85BrKi"
   },
   "source": [
    "# Experience Replay\n",
    "\n",
    "After defining the environment in the previous section you will now learn the mechanism of experience replaying and how the agent learns from these experiences. \n",
    "\n",
    "This notebook will cover:\n",
    "1. Definition of the memory or replay buffer\n",
    "2. Processing of experiences to create arrays target Q-values for agent learning\n",
    "\n",
    "In this notebook, you will perform the following steps:\n",
    "\n",
    "1. [Import Modules](#modules)\n",
    "2. [Code for replaying experiences](#exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXg7tDMTBrKk"
   },
   "source": [
    "## Import Modules\n",
    "\n",
    "In the code below you import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5W1wLgFSBrKr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkCJqybEBrK1"
   },
   "source": [
    "<a id='exp'></a> \n",
    "## Code for replaying experiences\n",
    "\n",
    "The code for experience replay has three essential functions:\n",
    "\n",
    "1. ```init()``` - which initialises the buffer and sets the maximum size of this buffer\n",
    "2. ```remember()``` - which adds new experiences to the buffer and truncates older ones\n",
    "3. ```process()``` - which returns the input state and target Q-values. This is so that we can update Q-values for a given state action pair while training the agent.\n",
    "\n",
    "The ```process()``` function is the most important of the three. The flow of the ```process()``` function as follows:\n",
    "\n",
    "1. You randomly select experiences from the memory buffer. This returns many entries with the structure of S.A.R.S. Which is state, action, reward and next state.\n",
    "2. For each such experience, you first take the state at time t. This state is used to get the Q-values from model R. These Q-values from model R is stored in a target vector. They tell you the current importance of each action in this state.\n",
    "3. Thereafter, you get the Q-value of the most optimal action using the model Q for the state at time t+1. The state at t+1 is the next state.\n",
    "4. This Q-value for the next state is discounted first. It is then added to the reward earned for the transition from the state at t and t+1.\n",
    "5. This summed value is then set as the Q-value for action taken at state at t in the target vector. This is the only action for which the Q-value is replaced. This is because we want to train the agent to learn the importance of this action based on the value of the next state.\n",
    "6. All such state (state at t) and target vectors are returned for all sampled experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trIH15efBrK3"
   },
   "outputs": [],
   "source": [
    "# The rate at which Q-values of subsequent states are discounted\n",
    "DISCOUNT_RATE = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7E6uhKvvBrK-"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    '''This class calculates the Q-Table.\n",
    "    It gathers memory from previous experience and \n",
    "    creates a Q-Table with states and rewards for each\n",
    "    action using the NN. At the end of the game the reward\n",
    "    is calculated from the reward function. \n",
    "    The weights in the NN are constantly updated with each new\n",
    "    batch of experience. \n",
    "    This is the heart of the RL algorithm.\n",
    "    Args:\n",
    "        state_tp1: state at time t+1\n",
    "        state_t: state at time t\n",
    "        action_t: int {0..2} hold, sell, buy taken at state_t \n",
    "        Q_sa: float, reward for state_tp1\n",
    "        reward_t: reward for state_t\n",
    "        self.memory: list of state_t, action_t and reward_t at time t as well as state_tp1\n",
    "        targets: array(float) Nx2, weight of each action \n",
    "        inputs: an array with scrambled states at different times\n",
    "        targets: Nx3 array of weights for each action for scrambled input states\n",
    "    '''\n",
    "    def __init__(self, max_memory=1000, discount=DISCOUNT_RATE):\n",
    "        # Set the length of the memoty buffer\n",
    "        self.max_memory = max_memory\n",
    "        # Initialise the memory as a list\n",
    "        self.memory = list()\n",
    "        # Set the reward and q-value tradeoff \n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # Add states to time t and t+1 as well as  to memory\n",
    "        self.memory.append([states, game_over])\n",
    "        # If entries added are more than max_memory\n",
    "        # truncate the first entry\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def process(self, modelQ, modelR, batch_size=10):\n",
    "        # Get the length of the memory filled in the buffer\n",
    "        len_memory = len(self.memory)\n",
    "        # Get the number of actions the agent \n",
    "        num_actions = modelQ.output_shape[-1]\n",
    "        # Get the shape of state\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        \n",
    "        # Initialise input and target arrays\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        # Step randomly through different places in the memory\n",
    "        # and scramble them into a new input array (inputs) with the\n",
    "        # length of the pre-defined batch size\n",
    "                    \n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):    \n",
    "            # Obtain the parameters for Bellman from memory,\n",
    "            # S.A.R.S: state, action, reward, new state\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            # Boolean flag to check if the game is over\n",
    "            game_over = self.memory[idx][1]\n",
    "            inputs[i] = state_t    \n",
    "            \n",
    "            # Calculate the targets for the state at time t\n",
    "            targets[i] = modelR.predict(state_t)[0]\n",
    "            \n",
    "            # Calculate the reward at time t+1 for action at time t\n",
    "            Q_sa = np.max(modelQ.predict(state_tp1)[0])\n",
    "           \n",
    "            if game_over:\n",
    "                # When game is over we have a definite reward\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # Update the part of the target for which action_t occured to new value\n",
    "                # Q_new(s,a) = reward_t + gamma * max_a' Q(s', a')\n",
    "                \n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXz_gJpGBrLF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ExperienceReplay at 0x20f46e5f1c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an instance of the experience replay class\n",
    "ER = ExperienceReplay()\n",
    "ER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkReyXaTBrLM"
   },
   "source": [
    "Once the memory buffer is filled. We use the ```process()``` function to generate a pairing of input states and target Q-values. These are used in batches to update the models Q and R. We will use these returned values in functions from subsequent sections.\n",
    "<br></br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Experience_replay.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
