{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jscV6ioarGE5"
   },
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDsTCatmrGE8"
   },
   "source": [
    "# Training the agent on the Game environment\n",
    "\n",
    "After defining the agent in the last section, in this section you will learn and define the flow of the training of the agent on the environment. \n",
    "\n",
    "This notebook will cover:\n",
    "1. Definition of episodes\n",
    "2. Usage of epsilon value\n",
    "3. Usage of experience replay buffer\n",
    "4. Batch training of agent on sampled experiences\n",
    "\n",
    "In this notebook, you will perform the following steps:\n",
    "\n",
    "1. [Import Modules](#modules)\n",
    "1. [Read data](#read)\n",
    "2. [Define training hyperparameters](#hyper)\n",
    "3. [Training the agent](#run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uedni_0rGE9"
   },
   "source": [
    "<a id='modules'></a> \n",
    "## Import modules\n",
    "\n",
    "In the code below we import the modules. We import the Game and Experience classes from the quantra_reinforcement_learning module. We also import the module which initialises the agent, init_net. We had seen these in the previous sections.\n",
    "\n",
    "You can find the quantra_reinforcement_learning module from the last section of this course '**Python Codes and Data**' unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9jmjeK9rrGFy",
    "outputId": "5e95340f-abc0-4c22-a7da-76b4836514a8"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Appends new file paths to import modules\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# To suppress GPU related warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from data_modules.quantra_reinforcement_learning import reward_exponential_pnl\n",
    "from data_modules.quantra_reinforcement_learning import Game\n",
    "from data_modules.quantra_reinforcement_learning import init_net\n",
    "from data_modules.quantra_reinforcement_learning import ExperienceReplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsH7BcJrrGF-"
   },
   "source": [
    "<a id='read'></a> \n",
    "## Read price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pIOizgWFrGGJ",
    "outputId": "675a73bf-bf22-4747-e0fe-3477a66962c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:35:00-05:00</th>\n",
       "      <td>91.711</td>\n",
       "      <td>91.809</td>\n",
       "      <td>91.703</td>\n",
       "      <td>91.760</td>\n",
       "      <td>4448908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:40:00-05:00</th>\n",
       "      <td>91.752</td>\n",
       "      <td>91.973</td>\n",
       "      <td>91.752</td>\n",
       "      <td>91.932</td>\n",
       "      <td>4380988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:45:00-05:00</th>\n",
       "      <td>91.940</td>\n",
       "      <td>92.022</td>\n",
       "      <td>91.928</td>\n",
       "      <td>92.005</td>\n",
       "      <td>2876633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:50:00-05:00</th>\n",
       "      <td>92.005</td>\n",
       "      <td>92.177</td>\n",
       "      <td>91.973</td>\n",
       "      <td>92.177</td>\n",
       "      <td>4357079.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:55:00-05:00</th>\n",
       "      <td>92.168</td>\n",
       "      <td>92.177</td>\n",
       "      <td>92.038</td>\n",
       "      <td>92.079</td>\n",
       "      <td>2955068.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             open    high     low   close     volume\n",
       "Time                                                                \n",
       "2010-01-04 09:35:00-05:00  91.711  91.809  91.703  91.760  4448908.0\n",
       "2010-01-04 09:40:00-05:00  91.752  91.973  91.752  91.932  4380988.0\n",
       "2010-01-04 09:45:00-05:00  91.940  92.022  91.928  92.005  2876633.0\n",
       "2010-01-04 09:50:00-05:00  92.005  92.177  91.973  92.177  4357079.0\n",
       "2010-01-04 09:55:00-05:00  92.168  92.177  92.038  92.079  2955068.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data is stored in the directory 'data'\n",
    "path = '../data_modules/'\n",
    "\n",
    "bars5m = pd.read_pickle(path + 'PriceData5m.bz2')\n",
    "bars5m.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkC-9hsBrGKv"
   },
   "source": [
    "<a id='hyper'></a> \n",
    "## Define the training hyperparameters\n",
    "\n",
    "\n",
    "Below are the hyperparameters used by the ```run()``` function:\n",
    "\n",
    "1. EPSILON: the initial value of the policy probability epsilon is set.\n",
    "2. MAX_MEM:  maximum length of the experience replay buffer.\n",
    "3. BATCH_SIZE: number of experiences that need to be sampled from the experience replay buffer.\n",
    "4. LKBK: number of bars used as lookback for training.\n",
    "5. START_IDX: initial index of the dataset where the agent starts learning.\n",
    "6. EPS_MIN: This sets the minimum epsilon value. \n",
    "7. DISCOUNT_RATE: This sets the tradeoff fraction between reward and the Q-value of the next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tZZLMbQlrGKy"
   },
   "outputs": [],
   "source": [
    "rl_config = {\n",
    "\n",
    "    # LEARNING_RATE: the learning rate used in the algorithm's optimizer\n",
    "    'LEARNING_RATE': 0.05,\n",
    "\n",
    "    # LOSS_FUNCTION: the loss function used in the algorithm\n",
    "    'LOSS_FUNCTION': 'mse',\n",
    "\n",
    "    # ACTIVATION_FUN: the activation function used in the neural network model\n",
    "    'ACTIVATION_FUN': 'relu',\n",
    "\n",
    "    # NUM_ACTIONS: the number of actions that the agent can take in each state\n",
    "    'NUM_ACTIONS': 3,\n",
    "\n",
    "    # HIDDEN_MULT: a multiplier used to determine the size of the hidden layer in the neural network model\n",
    "    'HIDDEN_MULT': 2,\n",
    "\n",
    "    # DISCOUNT_RATE: the discount rate used in the algorithm\n",
    "    'DISCOUNT_RATE': 0.99,\n",
    "\n",
    "    # LKBK: the number of previous time steps to consider in the algorithm\n",
    "    'LKBK': 10,\n",
    "\n",
    "    # BATCH_SIZE: the size of the mini-batch used in the algorithm\n",
    "    'BATCH_SIZE': 1,\n",
    "\n",
    "    # MAX_MEM: the maximum size of the memory used in the algorithm\n",
    "    'MAX_MEM': 600,\n",
    "\n",
    "    # EPSILON: the initial value of epsilon used\n",
    "    'EPSILON': 0.01,\n",
    "\n",
    "    # EPS_MIN: the minimum value of epsilon used\n",
    "    'EPS_MIN': 0.001,\n",
    "\n",
    "    # START_IDX: the starting index used in the algorithm\n",
    "    'START_IDX': 3000,\n",
    "\n",
    "    # RF: the reward function used in the algorithm\n",
    "    'RF': reward_exponential_pnl,\n",
    "\n",
    "    # TEST_MODE: a boolean that indicates whether the algorithm is in test mode or not. Set TEST_MODE to False when running in the local system\n",
    "    'TEST_MODE': True,\n",
    "    \n",
    "    # PRELOAD: a boolean that indicates whether to preload the model from disk\n",
    "    'PRELOAD': False,\n",
    "    \n",
    "    # UPDATE_QR: a boolean that indicates whether to update the Q-values in the algorithm\n",
    "    'UPDATE_QR': True,\n",
    "    \n",
    "    # Saving the weights\n",
    "    'WEIGHTS_FILE': '../data_modules/indicator_model_fx_pair_0.h5',\n",
    "    \n",
    "    # Saving the trades\n",
    "    'TRADE_FILE': '../data_modules/trade_logs_fx_pair_0.bz2',\n",
    "    \n",
    "    # Experience replay\n",
    "    'REPLAY_FILE': '../data_modules/memory_fx_pair_0.bz2',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0esI35ncrGLZ"
   },
   "source": [
    "<a id='run'></a> \n",
    "## Training the agent\n",
    "\n",
    "In the code below you define the ```run()``` function. This function helps to train the agent in the Game environment. \n",
    "\n",
    "The logical flow of the agent training is:\n",
    "\n",
    "1. You first initialise the Game environment and the ANN agents. You also initialise the experience replay buffer.\n",
    "2. Each iteration of exploring the environment is called an episode. \n",
    "3. For each episode, you iterate over the states generated by the underlying OHLC data step by step.\n",
    "4. You use an exponential decay function to set a value called epsilon. Based on this value you decide to take an action with the maximum q-value (optimal) or a suboptimal action that you you want the agent to explore.\n",
    "5. Once the action is selected, take the action. This will give the next state and the reward from the environment.\n",
    "6. Add the experience [current state, action, reward, next state] to the experience replay buffer.\n",
    "7. Sample experiences from the experience replay buffer. Use these experiences to calculate the target q-value of a given state and action.\n",
    "8. Target is calculated using the two models: modelR and modelQ as covered in previous sections.\n",
    "8. Use this target q-value to train the agent and reduce loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dSRLycomrGLa"
   },
   "outputs": [],
   "source": [
    "def run(bars5m, rl_config):\n",
    "    \"\"\"\n",
    "    Function to run the RL model on the passed price data\n",
    "    \"\"\"\n",
    "    \n",
    "    pnls = []\n",
    "    trade_logs = pd.DataFrame()\n",
    "    episode = 0\n",
    "\n",
    "    ohlcv_dict = {\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    }\n",
    "\n",
    "    bars1h = bars5m.resample('1H', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "    bars1d = bars1h.resample('1D', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "\n",
    "    \"\"\"---Initialise a NN and a set up initial game parameters---\"\"\"\n",
    "    env = Game(bars5m, bars1d, bars1h, rl_config['RF'],\n",
    "               lkbk=rl_config['LKBK'], init_idx=rl_config['START_IDX'])\n",
    "    q_network, r_network = init_net(env, rl_config)\n",
    "    exp_replay = ExperienceReplay(max_memory=rl_config['MAX_MEM'], discount=rl_config['DISCOUNT_RATE'])\n",
    "\n",
    "    \"\"\"---Preloading the model weights---\"\"\"\n",
    "    if rl_config['PRELOAD']:\n",
    "        q_network.load_weights(rl_config['WEIGHTS_FILE'])\n",
    "        r_network.load_weights(rl_config['WEIGHTS_FILE'])\n",
    "        exp_replay.memory = pickle.load(open(rl_config['REPLAY_FILE'], 'rb'))\n",
    "\n",
    "    r_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    \"\"\"---Loop that steps through one trade (game) at a time---\"\"\"\n",
    "    while True:\n",
    "        \"\"\"---Stop the algo when end is near to avoid exception---\"\"\"\n",
    "        if env.curr_idx >= len(bars5m)-1:\n",
    "            break\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        \"\"\"---Initialise a new game---\"\"\"\n",
    "        env = Game(bars5m, bars1d, bars1h, rl_config['RF'],\n",
    "                   lkbk=rl_config['LKBK'], init_idx=env.curr_idx)\n",
    "        state_tp1 = env.get_state()\n",
    "\n",
    "        \"\"\"---Calculate epsilon for exploration vs exploitation random action generator---\"\"\"\n",
    "        epsilon = rl_config['EPSILON']**(np.log10(episode))+rl_config['EPS_MIN']\n",
    "\n",
    "        game_over = False\n",
    "        cnt = 0\n",
    "\n",
    "        \"\"\"---Walk through time steps starting from the end of the last game---\"\"\"\n",
    "        while not game_over:\n",
    "        \n",
    "            if env.curr_idx >= len(bars5m)-1:\n",
    "                break\n",
    "\n",
    "            cnt += 1\n",
    "            state_t = state_tp1\n",
    "\n",
    "            \"\"\"---Generate a random action or through q_network---\"\"\"\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, 3, size=1)[0]\n",
    "\n",
    "            else:\n",
    "                q = q_network.predict(state_t)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            \"\"\"---Updating the Game---\"\"\"\n",
    "            reward, game_over = env.act(action)\n",
    "\n",
    "            \"\"\"---Updating trade/position logs---\"\"\"\n",
    "            tl = [[env.curr_time, env.position, episode]]\n",
    "            if game_over:\n",
    "                tl = [[env.curr_time, 0, episode]]\n",
    "            trade_logs = trade_logs.append(tl)\n",
    "\n",
    "            \"\"\"---Move to next time step---\"\"\"\n",
    "            env.curr_idx += 1\n",
    "            state_tp1 = env.get_state()\n",
    "\n",
    "            \"\"\"---Adding state to memory---\"\"\"\n",
    "            exp_replay.remember(\n",
    "                [state_t, action, reward, state_tp1], game_over)\n",
    "\n",
    "            \"\"\"---Creating a new Q-Table---\"\"\"\n",
    "            inputs, targets = exp_replay.process(\n",
    "                q_network, r_network, batch_size=rl_config['BATCH_SIZE'])\n",
    "            env.pnl_sum = sum(pnls)\n",
    "\n",
    "            \"\"\"---Update the NN model with a new Q-Table\"\"\"\n",
    "            q_network.train_on_batch(inputs, targets)\n",
    "\n",
    "            if game_over and rl_config['UPDATE_QR']:\n",
    "                r_network.set_weights(q_network.get_weights())\n",
    "\n",
    "        pnls.append(env.pnl)\n",
    "\n",
    "        print(\"Trade {:03d} | pos {} | len {} | approx cum ret {:,.2f}% | trade ret {:,.2f}% | eps {:,.4f} | {} | {}\".format(\n",
    "            episode, env.position, env.trade_len, sum(pnls)*100, env.pnl*100, epsilon, env.curr_time, env.curr_idx))\n",
    "\n",
    "        if not episode % 10:\n",
    "            print('----saving weights, trade logs and replay buffer-----')\n",
    "            r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n",
    "            trade_logs.to_pickle(rl_config['TRADE_FILE'])\n",
    "            pickle.dump(exp_replay.memory, open(rl_config['REPLAY_FILE'], 'wb'))\n",
    "\n",
    "        if not episode % 7 and rl_config['TEST_MODE']:\n",
    "            print('\\n**********************************************\\nTest mode is on due to resource constraints and therefore stopped after 7 trades. \\nYou can trade on full dataset on your local computer and set TEST_MODE flag to False in rl_config dictionary. \\nThe full code file, quantra_reinforemcent_learning module and data file is available in last unit of the course.\\n**********************************************\\n')\n",
    "            break\n",
    "\n",
    "    if not rl_config['TEST_MODE']:\n",
    "        print('----saving weights, trade logs and replay buffer-----')\n",
    "        r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n",
    "        trade_logs.to_pickle(rl_config['TRADE_FILE'])\n",
    "        pickle.dump(exp_replay.memory, open(rl_config['REPLAY_FILE'], 'wb'))\n",
    "\n",
    "    print('***FINISHED***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OXKZD3DhrGLf",
    "outputId": "f26f8a0e-2270-40d9-bc38-d94ebadca9cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade 001 | pos 1 | len 2 | approx cum ret 0.04% | trade ret 0.04% | eps 1.0010 | 2010-03-01 12:50:00-05:00 | 3003\n",
      "Trade 002 | pos -1 | len 1 | approx cum ret 0.09% | trade ret 0.05% | eps 0.2510 | 2010-03-01 13:00:00-05:00 | 3005\n",
      "Trade 003 | pos 1 | len 15 | approx cum ret 0.22% | trade ret 0.13% | eps 0.1121 | 2010-03-01 14:20:00-05:00 | 3021\n",
      "Trade 004 | pos 1 | len 14 | approx cum ret 0.12% | trade ret -0.10% | eps 0.0635 | 2010-03-01 15:35:00-05:00 | 3036\n",
      "Trade 005 | pos 1 | len 2 | approx cum ret 0.10% | trade ret -0.02% | eps 0.0410 | 2010-03-01 15:50:00-05:00 | 3039\n",
      "Trade 006 | pos 1 | len 105 | approx cum ret 1.11% | trade ret 1.01% | eps 0.0288 | 2010-03-03 11:40:00-05:00 | 3145\n",
      "Trade 007 | pos 1 | len 131 | approx cum ret 1.52% | trade ret 0.41% | eps 0.0214 | 2010-03-05 09:40:00-05:00 | 3277\n",
      "\n",
      "**********************************************\n",
      "Test mode is on due to resource constraints and therefore stopped after 7 trades. \n",
      "You can trade on full dataset on your local computer and set TEST_MODE flag to False in rl_config dictionary. \n",
      "The full code file, quantra_reinforemcent_learning module and data file is available in last unit of the course.\n",
      "**********************************************\n",
      "\n",
      "***FINISHED***\n"
     ]
    }
   ],
   "source": [
    "# Call the run function and pass the dataframe and hyperparameters\n",
    "run(bars5m, rl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0J5wWx3rGLn"
   },
   "source": [
    "As we can see above, the PnL, epsilon, entry date and episode length are printed for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqTRiLkDrGLo"
   },
   "source": [
    "Here we defined the run function in which the agent gathers new experiences in the Game environment and learns from them. In the coming units, we will run and evaluate this function for synthetic as well as real market data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
