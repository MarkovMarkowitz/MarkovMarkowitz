{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB07qp5NwMdy"
   },
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. If you are new to Jupyter notebooks, please go through this introductory manual <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\">here</a>.\n",
    "1. Any changes made in this notebook would be lost after you close the browser window. **You can download the notebook to save your work on your PC.**\n",
    "1. Before running this notebook on your local PC:<br>\n",
    "i.  You need to set up a Python environment and the relevant packages on your local PC. To do so, go through the section on \"**Run Codes Locally on Your Machine**\" in the course.<br>\n",
    "ii. You need to **download the zip file available in the last unit** of this course. The zip file contains the data files and/or python modules that might be required to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ctQsPorwMdz"
   },
   "source": [
    "# Artificial Neural Networks Based Double Deep Q Learning Agents\n",
    "\n",
    "\n",
    "In a reinforcement learning problem, except for the environment, the agent is the other cardinal part. In deep reinforcement learning, the agent is modelled using Artificial Neural Networks (ANNs). In this notebook, you will look at the Keras definitions of the two identical ANN-based agents which are used for creating the two Q-tables for Double Deep Q Learning architecture. Both of them are trained using different samples. This is done so that they can later be compared for value overestimation. This helps stabilise the learning process and helps make it fast.\n",
    "\n",
    "In this notebook, you will perform the following steps:\n",
    "\n",
    "1. [Import Modules](#modules)\n",
    "1. [Read OHLCV data](#read)\n",
    "2. [ANN hyperparameters](#hyper)\n",
    "3. [Define ANNs for DDQN](#net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmKOHUBnwMd0"
   },
   "source": [
    "<a id='modules'></a> \n",
    "## Import modules\n",
    "\n",
    "First, we import the modules. We import the sequential model, the dense layer and stochastic gradient descent (sgd) optimizer from the Keras package. We also import the Game class from the quantra_reinforcement_learning module. \n",
    "\n",
    "You can find the `quantra_reinforcement_learning` module from the last section of this course '**Python Codes and Data**' unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gqoDnnWFwMd1",
    "outputId": "c7ea4bf1-505f-4600-80e1-b04db479e759"
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# To suppress GPU related warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import Sequential model\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "# Import dense layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Import stochastic gradient descent optimizer\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Appends new file paths to import modules\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from data_modules.quantra_reinforcement_learning import Game\n",
    "from data_modules.quantra_reinforcement_learning import reward_exponential_pnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa4tS_7ywMd7"
   },
   "source": [
    "<a id='read'></a> \n",
    "## Read price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L9UrVUqQwMd7",
    "outputId": "5e566dc5-d0b3-4d37-ce5e-216d80eab3a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:35:00-05:00</th>\n",
       "      <td>91.711</td>\n",
       "      <td>91.809</td>\n",
       "      <td>91.703</td>\n",
       "      <td>91.760</td>\n",
       "      <td>4448908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:40:00-05:00</th>\n",
       "      <td>91.752</td>\n",
       "      <td>91.973</td>\n",
       "      <td>91.752</td>\n",
       "      <td>91.932</td>\n",
       "      <td>4380988.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:45:00-05:00</th>\n",
       "      <td>91.940</td>\n",
       "      <td>92.022</td>\n",
       "      <td>91.928</td>\n",
       "      <td>92.005</td>\n",
       "      <td>2876633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:50:00-05:00</th>\n",
       "      <td>92.005</td>\n",
       "      <td>92.177</td>\n",
       "      <td>91.973</td>\n",
       "      <td>92.177</td>\n",
       "      <td>4357079.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04 09:55:00-05:00</th>\n",
       "      <td>92.168</td>\n",
       "      <td>92.177</td>\n",
       "      <td>92.038</td>\n",
       "      <td>92.079</td>\n",
       "      <td>2955068.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             open    high     low   close     volume\n",
       "Time                                                                \n",
       "2010-01-04 09:35:00-05:00  91.711  91.809  91.703  91.760  4448908.0\n",
       "2010-01-04 09:40:00-05:00  91.752  91.973  91.752  91.932  4380988.0\n",
       "2010-01-04 09:45:00-05:00  91.940  92.022  91.928  92.005  2876633.0\n",
       "2010-01-04 09:50:00-05:00  92.005  92.177  91.973  92.177  4357079.0\n",
       "2010-01-04 09:55:00-05:00  92.168  92.177  92.038  92.079  2955068.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data is stored in the directory 'data'\n",
    "path = '../data_modules/'\n",
    "\n",
    "# Read 5 mins price data\n",
    "bars5m = pd.read_pickle(path + 'PriceData5m.bz2')\n",
    "\n",
    "bars5m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "romCaStzwMeB"
   },
   "source": [
    "<a id='hyper'></a> \n",
    "## ANN hyperparameters \n",
    "\n",
    "In the initialisations below, we define the various hyperparameters used by the ANNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KhVc4dIFwMeC"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store the configuration\n",
    "rl_config = {}\n",
    "\n",
    "# LEARNING_RATE: This is the multiplier for the steps of the gradient\n",
    "# This tells us how fast the optimizer will reach an optima\n",
    "rl_config['LEARNING_RATE'] = 0.05\n",
    "\n",
    "# LOSS_FUNCTIO: The function to quantify how much predictions are away from ground truth\n",
    "rl_config['LOSS_FUNCTION'] = 'mse'\n",
    "\n",
    "# ACTIVATION_FUN: The function that adds non-linearity to predictions for fitting complex curves\n",
    "rl_config['ACTIVATION_FUN'] = 'relu'\n",
    "\n",
    "# NUM_ACTIONS: Number of actions the agent can take; Buy, sell and hold\n",
    "rl_config['NUM_ACTIONS'] = 3\n",
    "\n",
    "# BATCH_SIZE: The number of samples being trained on at a given time\n",
    "rl_config['BATCH_SIZE'] = 1\n",
    "\n",
    "# HIDDEN_MULT: Relative size of the input and the hidden layer\n",
    "rl_config['HIDDEN_MULT'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6g6f5EAwMeF"
   },
   "source": [
    "<a id='net'></a> \n",
    "## Define ANNs for DDQN\n",
    "\n",
    "In the code below we define two multi-layer perceptrons. In a multi-layer perceptron, the data is passed into the neural network only once. The error is calculated and the feedback is given back from the last to the first layers. There is only one propagation of error or feedback backwards. This is called backpropagation.\n",
    "\n",
    "The dimensions of the input are equal to the dimensions of the state of the environment. The dimensions of the outputs of the agent are equal to the number of actions the agent can take. In this case, it is: buy, sell and hold. You create the models by embedding three ```dense()``` layers in the ```sequential()``` model as shown below.\n",
    "\n",
    "Two models are defined because Double Deep Q Learning requires two Q-tables learning simultaneously. This helps in avoiding value overestimation as stated in the previous video unit which in turn leads to faster learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3DQn_q0jwMeG"
   },
   "outputs": [],
   "source": [
    "def init_net(env, rl_config):\n",
    "    \"\"\".\n",
    "    Args:\n",
    "        env: an instance of the Game class which is used to create the environment the agent explores\n",
    "    Returns:\n",
    "        modelR: the neural network for R-value table\n",
    "        modelQ : the neural network for Q-value table\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size = len(env.state)*rl_config['HIDDEN_MULT']\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "    # Define the sequential function which encapsulates the layers of the model\n",
    "    modelR = Sequential()\n",
    "\n",
    "    # Define a dense layer with input shape equal to the size of the state vector\n",
    "    modelR.add(Dense(len(env.state), input_shape=(\n",
    "        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "\n",
    "    # Define a dense hidden layer of input size hidden_size. The activation function used is relu\n",
    "    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "\n",
    "    # Define a dense layer with output of the size of the num_actions\n",
    "    # which is total number of possible actions. The activation used is softmax\n",
    "    modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    # Use the stochaistic gradient descent optimizer\n",
    "    modelR.compile(SGD(lr=rl_config['LEARNING_RATE']),\n",
    "                   loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "    modelQ = Sequential()\n",
    "    modelQ.add(Dense(len(env.state), input_shape=(\n",
    "        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n",
    "    modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n",
    "    modelQ.compile(SGD(lr=rl_config['LEARNING_RATE']),\n",
    "                   loss=rl_config['LOSS_FUNCTION'])\n",
    "\n",
    "    return modelR, modelQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jDrExhHcwMeJ",
    "outputId": "064708f8-d16c-45de-a7a3-aba5a890a6a8"
   },
   "outputs": [],
   "source": [
    "# START_IDX: This is the starting index for the main loop, allow enough for lkbk\n",
    "START_IDX = 3000\n",
    "\n",
    "# LKBK: This is the lookback period, e.g. a value of 10 means 10 mins, 10 hours and 10 days!\n",
    "LKBK = 10\n",
    "\n",
    "ohlcv_dict = {\n",
    "    'open': 'first',\n",
    "    'high': 'max',\n",
    "    'low': 'min',\n",
    "    'close': 'last',\n",
    "    'volume': 'sum'\n",
    "}\n",
    "\n",
    "# Resample data to 1 hour data\n",
    "bars1h = bars5m.resample(\n",
    "    '1H', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "\n",
    "# Reample data to daily data\n",
    "bars1d = bars5m.resample(\n",
    "    '1D', label='left', closed='right').agg(ohlcv_dict).dropna()\n",
    "\n",
    "# Create the Game environment\n",
    "env = Game(bars5m, bars1d, bars1h, reward_exponential_pnl,\n",
    "           lkbk=LKBK, init_idx=START_IDX)\n",
    "\n",
    "# Create the model\n",
    "modelR, modelQ = init_net(env, rl_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nmKEIjeYwMeN",
    "outputId": "09c69c63-47e6-4ef7-bf29-43529af1ce65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 138)               19182     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 276)               38364     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 831       \n",
      "=================================================================\n",
      "Total params: 58,377\n",
      "Trainable params: 58,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelR.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dFL5r6qMwMeT",
    "outputId": "bd9eca5c-6d1b-4d21-a34a-9ccddc83aff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 138)               19182     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 276)               38364     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 831       \n",
      "=================================================================\n",
      "Total params: 58,377\n",
      "Trainable params: 58,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelQ.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLGZP_O7wMeX"
   },
   "source": [
    "You can choose to modify the agent definition. As stated before, you can try out with much more complex layers like LSTMs and 1D CNNs. In the coming units, you will learn how these ANNs are trained using a method known as experience replay.  <br><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Agents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
